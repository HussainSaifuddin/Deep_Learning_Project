{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi_class Classification Iris Flowers Project :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# objective:Botanist wants to determine the species of an iris flower based on characteristics of that flower.For instance attributes including petal,petal width,sepal length,sepal width are \"features\" that determine the classification of a given iris flower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start off by importing all of the classes and functions we will need. This includes both the functionality we require from Keras, but also data loading from pandas as well as data preparation and model evaluation from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the iris dataset  into scikit_learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import load_iris function from dataset module\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save bunch object containing iris_dataset and iris attributes\n",
    "data_frame_iris  = load_iris()\n",
    "type(data_frame_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n"
     ]
    }
   ],
   "source": [
    "# print the iris data\n",
    "print(data_frame_iris.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning Termonolgy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " •\tEach row is an observation is (also known as: samples, instance   ).\n",
    " \n",
    " •\tEach column is a feature (also know as : predicator, attributes, input ,covariate).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "# print the names of features\n",
    "print(data_frame_iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# print encoding scheme for sepecies: 0 = \"setosa\", 1 = \"versicolor\", 2 = \"virginica\"\n",
    "print(data_frame_iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "# print the integers the species of each observation\n",
    "print(data_frame_iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " •\tEach  value we are predicting is the response (also known as: target ,outcomes,label,dependent variable).\n",
    " \n",
    " •\tClassification is supervised learning in which the response is categorical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# check the type of featurs and response\n",
    "print(type(data_frame_iris.data))\n",
    "print(type(data_frame_iris.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "# check the shape of features and response\n",
    "print(data_frame_iris.data.shape)\n",
    "print(data_frame_iris.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n"
     ]
    }
   ],
   "source": [
    "# store feature matrix in x\n",
    "x =data_frame_iris[\"data\"]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# store response matrix in y\n",
    "y =to_categorical(data_frame_iris[\"target\"])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data  and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.5 2.6 4.4 1.2]\n",
      " [6.7 3.  5.  1.7]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.  2.2 5.  1.5]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [5.  3.  1.6 0.2]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [5.5 3.5 1.3 0.2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_train)\n",
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.9 3.2 5.7 2.3]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [6.  3.  4.8 1.8]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [6.1 3.  4.9 1.8]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_test)\n",
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train)\n",
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_test)\n",
    "len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0714 16:41:29.817215  3116 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0714 16:41:29.880045  3116 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0714 16:41:29.892033  3116 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(6,input_dim = 4 ,activation = \"relu\"))\n",
    "model.add(layers.Dense(3,activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0714 16:41:29.967398  3116 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0714 16:41:30.011379  3116 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0714 16:41:30.244799  3116 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0714 16:41:30.379394  3116 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 45 samples\n",
      "Epoch 1/200\n",
      "105/105 [==============================] - 0s 3ms/step - loss: 1.3759 - acc: 0.2000 - val_loss: 1.1718 - val_acc: 0.1111\n",
      "Epoch 2/200\n",
      "105/105 [==============================] - 0s 275us/step - loss: 1.2342 - acc: 0.1810 - val_loss: 1.1182 - val_acc: 0.3111\n",
      "Epoch 3/200\n",
      "105/105 [==============================] - 0s 370us/step - loss: 1.1767 - acc: 0.2571 - val_loss: 1.0861 - val_acc: 0.3556\n",
      "Epoch 4/200\n",
      "105/105 [==============================] - 0s 376us/step - loss: 1.1333 - acc: 0.3143 - val_loss: 1.0660 - val_acc: 0.3778\n",
      "Epoch 5/200\n",
      "105/105 [==============================] - 0s 370us/step - loss: 1.0978 - acc: 0.3333 - val_loss: 1.0413 - val_acc: 0.3778\n",
      "Epoch 6/200\n",
      "105/105 [==============================] - 0s 412us/step - loss: 1.0750 - acc: 0.3333 - val_loss: 1.0197 - val_acc: 0.3778\n",
      "Epoch 7/200\n",
      "105/105 [==============================] - 0s 378us/step - loss: 1.0474 - acc: 0.3333 - val_loss: 0.9961 - val_acc: 0.3778\n",
      "Epoch 8/200\n",
      "105/105 [==============================] - 0s 378us/step - loss: 1.0266 - acc: 0.3429 - val_loss: 0.9751 - val_acc: 0.3778\n",
      "Epoch 9/200\n",
      "105/105 [==============================] - 0s 408us/step - loss: 0.9980 - acc: 0.3429 - val_loss: 0.9533 - val_acc: 0.4444\n",
      "Epoch 10/200\n",
      "105/105 [==============================] - 0s 342us/step - loss: 0.9721 - acc: 0.3810 - val_loss: 0.9347 - val_acc: 0.5333\n",
      "Epoch 11/200\n",
      "105/105 [==============================] - 0s 313us/step - loss: 0.9495 - acc: 0.4762 - val_loss: 0.9085 - val_acc: 0.5333\n",
      "Epoch 12/200\n",
      "105/105 [==============================] - 0s 408us/step - loss: 0.9268 - acc: 0.4476 - val_loss: 0.8839 - val_acc: 0.5778\n",
      "Epoch 13/200\n",
      "105/105 [==============================] - 0s 339us/step - loss: 0.9023 - acc: 0.4571 - val_loss: 0.8687 - val_acc: 0.6000\n",
      "Epoch 14/200\n",
      "105/105 [==============================] - 0s 400us/step - loss: 0.8820 - acc: 0.4952 - val_loss: 0.8438 - val_acc: 0.6000\n",
      "Epoch 15/200\n",
      "105/105 [==============================] - 0s 294us/step - loss: 0.8601 - acc: 0.5143 - val_loss: 0.8191 - val_acc: 0.6889\n",
      "Epoch 16/200\n",
      "105/105 [==============================] - 0s 347us/step - loss: 0.8376 - acc: 0.6190 - val_loss: 0.7998 - val_acc: 0.7333\n",
      "Epoch 17/200\n",
      "105/105 [==============================] - 0s 523us/step - loss: 0.8158 - acc: 0.7333 - val_loss: 0.7732 - val_acc: 0.8444\n",
      "Epoch 18/200\n",
      "105/105 [==============================] - 0s 474us/step - loss: 0.7989 - acc: 0.7333 - val_loss: 0.7572 - val_acc: 0.8667\n",
      "Epoch 19/200\n",
      "105/105 [==============================] - 0s 316us/step - loss: 0.7797 - acc: 0.8381 - val_loss: 0.7369 - val_acc: 0.9111\n",
      "Epoch 20/200\n",
      "105/105 [==============================] - 0s 367us/step - loss: 0.7604 - acc: 0.8571 - val_loss: 0.7189 - val_acc: 0.9111\n",
      "Epoch 21/200\n",
      "105/105 [==============================] - 0s 335us/step - loss: 0.7444 - acc: 0.8952 - val_loss: 0.7021 - val_acc: 0.9333\n",
      "Epoch 22/200\n",
      "105/105 [==============================] - 0s 299us/step - loss: 0.7268 - acc: 0.8952 - val_loss: 0.6847 - val_acc: 0.9333\n",
      "Epoch 23/200\n",
      "105/105 [==============================] - 0s 323us/step - loss: 0.7091 - acc: 0.8857 - val_loss: 0.6670 - val_acc: 0.9333\n",
      "Epoch 24/200\n",
      "105/105 [==============================] - 0s 306us/step - loss: 0.6943 - acc: 0.8286 - val_loss: 0.6537 - val_acc: 0.9556\n",
      "Epoch 25/200\n",
      "105/105 [==============================] - 0s 294us/step - loss: 0.6766 - acc: 0.9238 - val_loss: 0.6358 - val_acc: 0.9556\n",
      "Epoch 26/200\n",
      "105/105 [==============================] - 0s 324us/step - loss: 0.6636 - acc: 0.9333 - val_loss: 0.6174 - val_acc: 0.9111\n",
      "Epoch 27/200\n",
      "105/105 [==============================] - 0s 348us/step - loss: 0.6488 - acc: 0.8476 - val_loss: 0.6140 - val_acc: 0.9556\n",
      "Epoch 28/200\n",
      "105/105 [==============================] - 0s 323us/step - loss: 0.6374 - acc: 0.9524 - val_loss: 0.6025 - val_acc: 0.9556\n",
      "Epoch 29/200\n",
      "105/105 [==============================] - 0s 307us/step - loss: 0.6249 - acc: 0.9524 - val_loss: 0.5876 - val_acc: 0.9556\n",
      "Epoch 30/200\n",
      "105/105 [==============================] - 0s 320us/step - loss: 0.6134 - acc: 0.9238 - val_loss: 0.5775 - val_acc: 0.9556\n",
      "Epoch 31/200\n",
      "105/105 [==============================] - 0s 256us/step - loss: 0.6017 - acc: 0.9524 - val_loss: 0.5615 - val_acc: 0.9556\n",
      "Epoch 32/200\n",
      "105/105 [==============================] - 0s 256us/step - loss: 0.5894 - acc: 0.8667 - val_loss: 0.5584 - val_acc: 0.9778\n",
      "Epoch 33/200\n",
      "105/105 [==============================] - 0s 294us/step - loss: 0.5789 - acc: 0.9619 - val_loss: 0.5421 - val_acc: 0.9556\n",
      "Epoch 34/200\n",
      "105/105 [==============================] - 0s 275us/step - loss: 0.5674 - acc: 0.9524 - val_loss: 0.5298 - val_acc: 0.9556\n",
      "Epoch 35/200\n",
      "105/105 [==============================] - 0s 347us/step - loss: 0.5591 - acc: 0.9429 - val_loss: 0.5241 - val_acc: 0.9556\n",
      "Epoch 36/200\n",
      "105/105 [==============================] - 0s 294us/step - loss: 0.5500 - acc: 0.9524 - val_loss: 0.5127 - val_acc: 0.9556\n",
      "Epoch 37/200\n",
      "105/105 [==============================] - 0s 285us/step - loss: 0.5418 - acc: 0.9524 - val_loss: 0.5030 - val_acc: 0.9556\n",
      "Epoch 38/200\n",
      "105/105 [==============================] - 0s 290us/step - loss: 0.5336 - acc: 0.9238 - val_loss: 0.4998 - val_acc: 0.9556\n",
      "Epoch 39/200\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.5239 - acc: 0.9714 - val_loss: 0.4900 - val_acc: 0.9556\n",
      "Epoch 40/200\n",
      "105/105 [==============================] - 0s 256us/step - loss: 0.5168 - acc: 0.9524 - val_loss: 0.4798 - val_acc: 0.9556\n",
      "Epoch 41/200\n",
      "105/105 [==============================] - 0s 290us/step - loss: 0.5068 - acc: 0.9238 - val_loss: 0.4811 - val_acc: 1.0000\n",
      "Epoch 42/200\n",
      "105/105 [==============================] - 0s 342us/step - loss: 0.5003 - acc: 0.9619 - val_loss: 0.4745 - val_acc: 0.9778\n",
      "Epoch 43/200\n",
      "105/105 [==============================] - 0s 237us/step - loss: 0.4921 - acc: 0.9619 - val_loss: 0.4673 - val_acc: 0.9778\n",
      "Epoch 44/200\n",
      "105/105 [==============================] - 0s 218us/step - loss: 0.4845 - acc: 0.9429 - val_loss: 0.4560 - val_acc: 1.0000\n",
      "Epoch 45/200\n",
      "105/105 [==============================] - 0s 208us/step - loss: 0.4780 - acc: 0.9619 - val_loss: 0.4466 - val_acc: 0.9556\n",
      "Epoch 46/200\n",
      "105/105 [==============================] - 0s 248us/step - loss: 0.4706 - acc: 0.9619 - val_loss: 0.4412 - val_acc: 0.9556\n",
      "Epoch 47/200\n",
      "105/105 [==============================] - 0s 266us/step - loss: 0.4648 - acc: 0.9524 - val_loss: 0.4371 - val_acc: 1.0000\n",
      "Epoch 48/200\n",
      "105/105 [==============================] - 0s 238us/step - loss: 0.4569 - acc: 0.9524 - val_loss: 0.4268 - val_acc: 0.9556\n",
      "Epoch 49/200\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.4522 - acc: 0.9524 - val_loss: 0.4251 - val_acc: 1.0000\n",
      "Epoch 50/200\n",
      "105/105 [==============================] - 0s 209us/step - loss: 0.4459 - acc: 0.9619 - val_loss: 0.4178 - val_acc: 1.0000\n",
      "Epoch 51/200\n",
      "105/105 [==============================] - 0s 294us/step - loss: 0.4418 - acc: 0.9524 - val_loss: 0.4155 - val_acc: 1.0000\n",
      "Epoch 52/200\n",
      "105/105 [==============================] - 0s 233us/step - loss: 0.4354 - acc: 0.9524 - val_loss: 0.4083 - val_acc: 0.9778\n",
      "Epoch 53/200\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.4290 - acc: 0.9619 - val_loss: 0.4038 - val_acc: 1.0000\n",
      "Epoch 54/200\n",
      "105/105 [==============================] - 0s 210us/step - loss: 0.4248 - acc: 0.9714 - val_loss: 0.4006 - val_acc: 1.0000\n",
      "Epoch 55/200\n",
      "105/105 [==============================] - 0s 285us/step - loss: 0.4191 - acc: 0.9524 - val_loss: 0.3973 - val_acc: 0.9778\n",
      "Epoch 56/200\n",
      "105/105 [==============================] - 0s 313us/step - loss: 0.4152 - acc: 0.9619 - val_loss: 0.3929 - val_acc: 0.9778\n",
      "Epoch 57/200\n",
      "105/105 [==============================] - 0s 275us/step - loss: 0.4091 - acc: 0.9524 - val_loss: 0.3910 - val_acc: 0.9778\n",
      "Epoch 58/200\n",
      "105/105 [==============================] - 0s 332us/step - loss: 0.4031 - acc: 0.9619 - val_loss: 0.3890 - val_acc: 0.9778\n",
      "Epoch 59/200\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.4002 - acc: 0.9714 - val_loss: 0.3783 - val_acc: 0.9778\n",
      "Epoch 60/200\n",
      "105/105 [==============================] - 0s 304us/step - loss: 0.3949 - acc: 0.9619 - val_loss: 0.3735 - val_acc: 0.9778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "105/105 [==============================] - 0s 279us/step - loss: 0.3905 - acc: 0.9524 - val_loss: 0.3694 - val_acc: 0.9778\n",
      "Epoch 62/200\n",
      "105/105 [==============================] - 0s 199us/step - loss: 0.3850 - acc: 0.9714 - val_loss: 0.3691 - val_acc: 0.9778\n",
      "Epoch 63/200\n",
      "105/105 [==============================] - 0s 294us/step - loss: 0.3797 - acc: 0.9714 - val_loss: 0.3634 - val_acc: 0.9778\n",
      "Epoch 64/200\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.3755 - acc: 0.9714 - val_loss: 0.3608 - val_acc: 0.9778\n",
      "Epoch 65/200\n",
      "105/105 [==============================] - 0s 283us/step - loss: 0.3709 - acc: 0.9524 - val_loss: 0.3555 - val_acc: 0.9778\n",
      "Epoch 66/200\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.3664 - acc: 0.9714 - val_loss: 0.3488 - val_acc: 0.9778\n",
      "Epoch 67/200\n",
      "105/105 [==============================] - 0s 255us/step - loss: 0.3618 - acc: 0.9714 - val_loss: 0.3470 - val_acc: 0.9778\n",
      "Epoch 68/200\n",
      "105/105 [==============================] - 0s 257us/step - loss: 0.3567 - acc: 0.9714 - val_loss: 0.3432 - val_acc: 0.9778\n",
      "Epoch 69/200\n",
      "105/105 [==============================] - 0s 294us/step - loss: 0.3549 - acc: 0.9619 - val_loss: 0.3386 - val_acc: 0.9778\n",
      "Epoch 70/200\n",
      "105/105 [==============================] - 0s 294us/step - loss: 0.3479 - acc: 0.9524 - val_loss: 0.3343 - val_acc: 0.9778\n",
      "Epoch 71/200\n",
      "105/105 [==============================] - 0s 309us/step - loss: 0.3442 - acc: 0.9714 - val_loss: 0.3290 - val_acc: 0.9778\n",
      "Epoch 72/200\n",
      "105/105 [==============================] - 0s 304us/step - loss: 0.3404 - acc: 0.9619 - val_loss: 0.3277 - val_acc: 0.9778\n",
      "Epoch 73/200\n",
      "105/105 [==============================] - 0s 266us/step - loss: 0.3350 - acc: 0.9619 - val_loss: 0.3303 - val_acc: 0.9778\n",
      "Epoch 74/200\n",
      "105/105 [==============================] - 0s 389us/step - loss: 0.3324 - acc: 0.9619 - val_loss: 0.3201 - val_acc: 0.9778\n",
      "Epoch 75/200\n",
      "105/105 [==============================] - 0s 285us/step - loss: 0.3280 - acc: 0.9619 - val_loss: 0.3162 - val_acc: 0.9778\n",
      "Epoch 76/200\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.3227 - acc: 0.9714 - val_loss: 0.3162 - val_acc: 0.9778\n",
      "Epoch 77/200\n",
      "105/105 [==============================] - 0s 236us/step - loss: 0.3196 - acc: 0.9619 - val_loss: 0.3141 - val_acc: 0.9778\n",
      "Epoch 78/200\n",
      "105/105 [==============================] - 0s 256us/step - loss: 0.3145 - acc: 0.9619 - val_loss: 0.3085 - val_acc: 0.9778\n",
      "Epoch 79/200\n",
      "105/105 [==============================] - 0s 237us/step - loss: 0.3099 - acc: 0.9714 - val_loss: 0.3127 - val_acc: 0.9778\n",
      "Epoch 80/200\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.3072 - acc: 0.9619 - val_loss: 0.3096 - val_acc: 0.9778\n",
      "Epoch 81/200\n",
      "105/105 [==============================] - 0s 266us/step - loss: 0.3044 - acc: 0.9619 - val_loss: 0.3044 - val_acc: 0.9778\n",
      "Epoch 82/200\n",
      "105/105 [==============================] - 0s 285us/step - loss: 0.3018 - acc: 0.9619 - val_loss: 0.2985 - val_acc: 0.9778\n",
      "Epoch 83/200\n",
      "105/105 [==============================] - 0s 275us/step - loss: 0.2955 - acc: 0.9619 - val_loss: 0.2932 - val_acc: 0.9778\n",
      "Epoch 84/200\n",
      "105/105 [==============================] - 0s 257us/step - loss: 0.2919 - acc: 0.9619 - val_loss: 0.2955 - val_acc: 0.9778\n",
      "Epoch 85/200\n",
      "105/105 [==============================] - 0s 265us/step - loss: 0.2890 - acc: 0.9619 - val_loss: 0.2901 - val_acc: 0.9778\n",
      "Epoch 86/200\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2844 - acc: 0.9619 - val_loss: 0.2825 - val_acc: 0.9778\n",
      "Epoch 87/200\n",
      "105/105 [==============================] - 0s 266us/step - loss: 0.2813 - acc: 0.9619 - val_loss: 0.2750 - val_acc: 0.9778\n",
      "Epoch 88/200\n",
      "105/105 [==============================] - 0s 323us/step - loss: 0.2783 - acc: 0.9714 - val_loss: 0.2703 - val_acc: 0.9778\n",
      "Epoch 89/200\n",
      "105/105 [==============================] - 0s 268us/step - loss: 0.2742 - acc: 0.9619 - val_loss: 0.2694 - val_acc: 0.9778\n",
      "Epoch 90/200\n",
      "105/105 [==============================] - 0s 238us/step - loss: 0.2704 - acc: 0.9714 - val_loss: 0.2684 - val_acc: 0.9778\n",
      "Epoch 91/200\n",
      "105/105 [==============================] - 0s 199us/step - loss: 0.2663 - acc: 0.9619 - val_loss: 0.2599 - val_acc: 0.9778\n",
      "Epoch 92/200\n",
      "105/105 [==============================] - 0s 257us/step - loss: 0.2651 - acc: 0.9619 - val_loss: 0.2638 - val_acc: 0.9778\n",
      "Epoch 93/200\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.2615 - acc: 0.9619 - val_loss: 0.2626 - val_acc: 0.9778\n",
      "Epoch 94/200\n",
      "105/105 [==============================] - 0s 409us/step - loss: 0.2570 - acc: 0.9714 - val_loss: 0.2626 - val_acc: 0.9778\n",
      "Epoch 95/200\n",
      "105/105 [==============================] - 0s 342us/step - loss: 0.2550 - acc: 0.9619 - val_loss: 0.2587 - val_acc: 0.9778\n",
      "Epoch 96/200\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.2516 - acc: 0.9619 - val_loss: 0.2541 - val_acc: 0.9778\n",
      "Epoch 97/200\n",
      "105/105 [==============================] - 0s 304us/step - loss: 0.2481 - acc: 0.9619 - val_loss: 0.2540 - val_acc: 0.9778\n",
      "Epoch 98/200\n",
      "105/105 [==============================] - 0s 218us/step - loss: 0.2468 - acc: 0.9619 - val_loss: 0.2453 - val_acc: 0.9778\n",
      "Epoch 99/200\n",
      "105/105 [==============================] - 0s 217us/step - loss: 0.2428 - acc: 0.9714 - val_loss: 0.2413 - val_acc: 0.9778\n",
      "Epoch 100/200\n",
      "105/105 [==============================] - 0s 206us/step - loss: 0.2392 - acc: 0.9714 - val_loss: 0.2412 - val_acc: 0.9778\n",
      "Epoch 101/200\n",
      "105/105 [==============================] - 0s 194us/step - loss: 0.2370 - acc: 0.9619 - val_loss: 0.2403 - val_acc: 0.9778\n",
      "Epoch 102/200\n",
      "105/105 [==============================] - 0s 218us/step - loss: 0.2350 - acc: 0.9619 - val_loss: 0.2470 - val_acc: 0.9778\n",
      "Epoch 103/200\n",
      "105/105 [==============================] - 0s 256us/step - loss: 0.2312 - acc: 0.9619 - val_loss: 0.2412 - val_acc: 0.9778\n",
      "Epoch 104/200\n",
      "105/105 [==============================] - 0s 207us/step - loss: 0.2289 - acc: 0.9619 - val_loss: 0.2317 - val_acc: 0.9778\n",
      "Epoch 105/200\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.2273 - acc: 0.9714 - val_loss: 0.2285 - val_acc: 0.9778\n",
      "Epoch 106/200\n",
      "105/105 [==============================] - 0s 237us/step - loss: 0.2223 - acc: 0.9714 - val_loss: 0.2403 - val_acc: 0.9778\n",
      "Epoch 107/200\n",
      "105/105 [==============================] - 0s 218us/step - loss: 0.2214 - acc: 0.9619 - val_loss: 0.2251 - val_acc: 0.9778\n",
      "Epoch 108/200\n",
      "105/105 [==============================] - 0s 214us/step - loss: 0.2198 - acc: 0.9619 - val_loss: 0.2263 - val_acc: 0.9778\n",
      "Epoch 109/200\n",
      "105/105 [==============================] - 0s 209us/step - loss: 0.2154 - acc: 0.9619 - val_loss: 0.2327 - val_acc: 0.9778\n",
      "Epoch 110/200\n",
      "105/105 [==============================] - 0s 237us/step - loss: 0.2136 - acc: 0.9619 - val_loss: 0.2258 - val_acc: 0.9778\n",
      "Epoch 111/200\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.2105 - acc: 0.9619 - val_loss: 0.2252 - val_acc: 0.9778\n",
      "Epoch 112/200\n",
      "105/105 [==============================] - 0s 219us/step - loss: 0.2112 - acc: 0.9619 - val_loss: 0.2241 - val_acc: 0.9778\n",
      "Epoch 113/200\n",
      "105/105 [==============================] - 0s 199us/step - loss: 0.2057 - acc: 0.9619 - val_loss: 0.2105 - val_acc: 0.9778\n",
      "Epoch 114/200\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.2039 - acc: 0.9714 - val_loss: 0.2165 - val_acc: 0.9778\n",
      "Epoch 115/200\n",
      "105/105 [==============================] - 0s 237us/step - loss: 0.2018 - acc: 0.9714 - val_loss: 0.2178 - val_acc: 0.9778\n",
      "Epoch 116/200\n",
      "105/105 [==============================] - 0s 222us/step - loss: 0.2001 - acc: 0.9619 - val_loss: 0.2056 - val_acc: 0.9778\n",
      "Epoch 117/200\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.1975 - acc: 0.9619 - val_loss: 0.2013 - val_acc: 0.9778\n",
      "Epoch 118/200\n",
      "105/105 [==============================] - 0s 237us/step - loss: 0.1941 - acc: 0.9619 - val_loss: 0.2127 - val_acc: 0.9778\n",
      "Epoch 119/200\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.1926 - acc: 0.9714 - val_loss: 0.2201 - val_acc: 0.9556\n",
      "Epoch 120/200\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.1914 - acc: 0.9619 - val_loss: 0.2020 - val_acc: 0.9778\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 197us/step - loss: 0.1898 - acc: 0.9619 - val_loss: 0.2035 - val_acc: 0.9778\n",
      "Epoch 122/200\n",
      "105/105 [==============================] - 0s 218us/step - loss: 0.1862 - acc: 0.9619 - val_loss: 0.1940 - val_acc: 0.9778\n",
      "Epoch 123/200\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.1844 - acc: 0.9619 - val_loss: 0.1917 - val_acc: 0.9778\n",
      "Epoch 124/200\n",
      "105/105 [==============================] - 0s 242us/step - loss: 0.1837 - acc: 0.9714 - val_loss: 0.1922 - val_acc: 0.9778\n",
      "Epoch 125/200\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.1817 - acc: 0.9619 - val_loss: 0.1872 - val_acc: 0.9778\n",
      "Epoch 126/200\n",
      "105/105 [==============================] - 0s 285us/step - loss: 0.1791 - acc: 0.9619 - val_loss: 0.1870 - val_acc: 0.9778\n",
      "Epoch 127/200\n",
      "105/105 [==============================] - 0s 296us/step - loss: 0.1788 - acc: 0.9714 - val_loss: 0.1936 - val_acc: 0.9778\n",
      "Epoch 128/200\n",
      "105/105 [==============================] - 0s 256us/step - loss: 0.1779 - acc: 0.9619 - val_loss: 0.1874 - val_acc: 0.9778\n",
      "Epoch 129/200\n",
      "105/105 [==============================] - 0s 342us/step - loss: 0.1731 - acc: 0.9619 - val_loss: 0.1849 - val_acc: 0.9778\n",
      "Epoch 130/200\n",
      "105/105 [==============================] - 0s 245us/step - loss: 0.1720 - acc: 0.9714 - val_loss: 0.1917 - val_acc: 0.9778\n",
      "Epoch 131/200\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.1710 - acc: 0.9619 - val_loss: 0.1867 - val_acc: 0.9778\n",
      "Epoch 132/200\n",
      "105/105 [==============================] - 0s 218us/step - loss: 0.1682 - acc: 0.9619 - val_loss: 0.1893 - val_acc: 0.9778\n",
      "Epoch 133/200\n",
      "105/105 [==============================] - 0s 275us/step - loss: 0.1669 - acc: 0.9619 - val_loss: 0.1734 - val_acc: 0.9778\n",
      "Epoch 134/200\n",
      "105/105 [==============================] - 0s 288us/step - loss: 0.1660 - acc: 0.9714 - val_loss: 0.1731 - val_acc: 0.9778\n",
      "Epoch 135/200\n",
      "105/105 [==============================] - 0s 267us/step - loss: 0.1655 - acc: 0.9714 - val_loss: 0.1773 - val_acc: 0.9778\n",
      "Epoch 136/200\n",
      "105/105 [==============================] - 0s 275us/step - loss: 0.1631 - acc: 0.9619 - val_loss: 0.1809 - val_acc: 0.9778\n",
      "Epoch 137/200\n",
      "105/105 [==============================] - 0s 338us/step - loss: 0.1609 - acc: 0.9619 - val_loss: 0.1776 - val_acc: 0.9778\n",
      "Epoch 138/200\n",
      "105/105 [==============================] - 0s 256us/step - loss: 0.1592 - acc: 0.9619 - val_loss: 0.1707 - val_acc: 0.9778\n",
      "Epoch 139/200\n",
      "105/105 [==============================] - 0s 256us/step - loss: 0.1585 - acc: 0.9619 - val_loss: 0.1713 - val_acc: 0.9778\n",
      "Epoch 140/200\n",
      "105/105 [==============================] - 0s 276us/step - loss: 0.1573 - acc: 0.9714 - val_loss: 0.1728 - val_acc: 0.9778\n",
      "Epoch 141/200\n",
      "105/105 [==============================] - 0s 361us/step - loss: 0.1560 - acc: 0.9619 - val_loss: 0.1739 - val_acc: 0.9778\n",
      "Epoch 142/200\n",
      "105/105 [==============================] - 0s 275us/step - loss: 0.1540 - acc: 0.9619 - val_loss: 0.1795 - val_acc: 0.9778\n",
      "Epoch 143/200\n",
      "105/105 [==============================] - 0s 259us/step - loss: 0.1543 - acc: 0.9619 - val_loss: 0.1681 - val_acc: 0.9778\n",
      "Epoch 144/200\n",
      "105/105 [==============================] - 0s 209us/step - loss: 0.1517 - acc: 0.9619 - val_loss: 0.1660 - val_acc: 0.9778\n",
      "Epoch 145/200\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.1496 - acc: 0.9619 - val_loss: 0.1650 - val_acc: 0.9778\n",
      "Epoch 146/200\n",
      "105/105 [==============================] - 0s 237us/step - loss: 0.1513 - acc: 0.9619 - val_loss: 0.1604 - val_acc: 0.9778\n",
      "Epoch 147/200\n",
      "105/105 [==============================] - 0s 266us/step - loss: 0.1482 - acc: 0.9619 - val_loss: 0.1572 - val_acc: 0.9778\n",
      "Epoch 148/200\n",
      "105/105 [==============================] - 0s 361us/step - loss: 0.1478 - acc: 0.9619 - val_loss: 0.1599 - val_acc: 0.9778\n",
      "Epoch 149/200\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.1446 - acc: 0.9714 - val_loss: 0.1579 - val_acc: 0.9778\n",
      "Epoch 150/200\n",
      "105/105 [==============================] - 0s 257us/step - loss: 0.1452 - acc: 0.9619 - val_loss: 0.1659 - val_acc: 0.9778\n",
      "Epoch 151/200\n",
      "105/105 [==============================] - 0s 294us/step - loss: 0.1445 - acc: 0.9619 - val_loss: 0.1616 - val_acc: 0.9778\n",
      "Epoch 152/200\n",
      "105/105 [==============================] - 0s 304us/step - loss: 0.1424 - acc: 0.9714 - val_loss: 0.1692 - val_acc: 0.9778\n",
      "Epoch 153/200\n",
      "105/105 [==============================] - 0s 294us/step - loss: 0.1418 - acc: 0.9619 - val_loss: 0.1690 - val_acc: 0.9778\n",
      "Epoch 154/200\n",
      "105/105 [==============================] - 0s 237us/step - loss: 0.1415 - acc: 0.9619 - val_loss: 0.1566 - val_acc: 0.9778\n",
      "Epoch 155/200\n",
      "105/105 [==============================] - 0s 217us/step - loss: 0.1387 - acc: 0.9619 - val_loss: 0.1554 - val_acc: 0.9778\n",
      "Epoch 156/200\n",
      "105/105 [==============================] - 0s 278us/step - loss: 0.1380 - acc: 0.9619 - val_loss: 0.1506 - val_acc: 0.9778\n",
      "Epoch 157/200\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.1390 - acc: 0.9714 - val_loss: 0.1542 - val_acc: 0.9778\n",
      "Epoch 158/200\n",
      "105/105 [==============================] - 0s 209us/step - loss: 0.1367 - acc: 0.9619 - val_loss: 0.1471 - val_acc: 0.9778\n",
      "Epoch 159/200\n",
      "105/105 [==============================] - 0s 218us/step - loss: 0.1351 - acc: 0.9714 - val_loss: 0.1484 - val_acc: 0.9778\n",
      "Epoch 160/200\n",
      "105/105 [==============================] - 0s 204us/step - loss: 0.1332 - acc: 0.9714 - val_loss: 0.1582 - val_acc: 0.9778\n",
      "Epoch 161/200\n",
      "105/105 [==============================] - 0s 209us/step - loss: 0.1335 - acc: 0.9619 - val_loss: 0.1447 - val_acc: 0.9778\n",
      "Epoch 162/200\n",
      "105/105 [==============================] - 0s 249us/step - loss: 0.1343 - acc: 0.9619 - val_loss: 0.1422 - val_acc: 0.9778\n",
      "Epoch 163/200\n",
      "105/105 [==============================] - 0s 217us/step - loss: 0.1316 - acc: 0.9714 - val_loss: 0.1482 - val_acc: 0.9778\n",
      "Epoch 164/200\n",
      "105/105 [==============================] - 0s 290us/step - loss: 0.1309 - acc: 0.9619 - val_loss: 0.1406 - val_acc: 0.9778\n",
      "Epoch 165/200\n",
      "105/105 [==============================] - 0s 257us/step - loss: 0.1305 - acc: 0.9619 - val_loss: 0.1443 - val_acc: 0.9778\n",
      "Epoch 166/200\n",
      "105/105 [==============================] - 0s 219us/step - loss: 0.1290 - acc: 0.9619 - val_loss: 0.1432 - val_acc: 0.9778\n",
      "Epoch 167/200\n",
      "105/105 [==============================] - 0s 275us/step - loss: 0.1279 - acc: 0.9619 - val_loss: 0.1478 - val_acc: 0.9778\n",
      "Epoch 168/200\n",
      "105/105 [==============================] - 0s 237us/step - loss: 0.1286 - acc: 0.9619 - val_loss: 0.1393 - val_acc: 0.9778\n",
      "Epoch 169/200\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.1259 - acc: 0.9714 - val_loss: 0.1395 - val_acc: 0.9778\n",
      "Epoch 170/200\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.1251 - acc: 0.9714 - val_loss: 0.1437 - val_acc: 0.9778\n",
      "Epoch 171/200\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.1242 - acc: 0.9619 - val_loss: 0.1434 - val_acc: 0.9778\n",
      "Epoch 172/200\n",
      "105/105 [==============================] - 0s 256us/step - loss: 0.1249 - acc: 0.9619 - val_loss: 0.1376 - val_acc: 0.9778\n",
      "Epoch 173/200\n",
      "105/105 [==============================] - 0s 235us/step - loss: 0.1233 - acc: 0.9714 - val_loss: 0.1420 - val_acc: 0.9778\n",
      "Epoch 174/200\n",
      "105/105 [==============================] - 0s 214us/step - loss: 0.1229 - acc: 0.9619 - val_loss: 0.1407 - val_acc: 0.9778\n",
      "Epoch 175/200\n",
      "105/105 [==============================] - 0s 237us/step - loss: 0.1218 - acc: 0.9619 - val_loss: 0.1365 - val_acc: 0.9778\n",
      "Epoch 176/200\n",
      "105/105 [==============================] - 0s 218us/step - loss: 0.1219 - acc: 0.9714 - val_loss: 0.1411 - val_acc: 0.9778\n",
      "Epoch 177/200\n",
      "105/105 [==============================] - 0s 226us/step - loss: 0.1205 - acc: 0.9619 - val_loss: 0.1364 - val_acc: 0.9778\n",
      "Epoch 178/200\n",
      "105/105 [==============================] - 0s 246us/step - loss: 0.1202 - acc: 0.9619 - val_loss: 0.1410 - val_acc: 0.9778\n",
      "Epoch 179/200\n",
      "105/105 [==============================] - 0s 285us/step - loss: 0.1199 - acc: 0.9619 - val_loss: 0.1395 - val_acc: 0.9778\n",
      "Epoch 180/200\n",
      "105/105 [==============================] - 0s 301us/step - loss: 0.1171 - acc: 0.9619 - val_loss: 0.1282 - val_acc: 0.9778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/200\n",
      "105/105 [==============================] - 0s 221us/step - loss: 0.1194 - acc: 0.9714 - val_loss: 0.1351 - val_acc: 0.9778\n",
      "Epoch 182/200\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.1170 - acc: 0.9619 - val_loss: 0.1302 - val_acc: 0.9778\n",
      "Epoch 183/200\n",
      "105/105 [==============================] - 0s 275us/step - loss: 0.1180 - acc: 0.9619 - val_loss: 0.1335 - val_acc: 0.9778\n",
      "Epoch 184/200\n",
      "105/105 [==============================] - 0s 296us/step - loss: 0.1151 - acc: 0.9619 - val_loss: 0.1390 - val_acc: 0.9778\n",
      "Epoch 185/200\n",
      "105/105 [==============================] - 0s 272us/step - loss: 0.1167 - acc: 0.9619 - val_loss: 0.1294 - val_acc: 0.9778\n",
      "Epoch 186/200\n",
      "105/105 [==============================] - 0s 319us/step - loss: 0.1149 - acc: 0.9619 - val_loss: 0.1305 - val_acc: 0.9778\n",
      "Epoch 187/200\n",
      "105/105 [==============================] - 0s 266us/step - loss: 0.1131 - acc: 0.9714 - val_loss: 0.1353 - val_acc: 0.9778\n",
      "Epoch 188/200\n",
      "105/105 [==============================] - 0s 236us/step - loss: 0.1142 - acc: 0.9619 - val_loss: 0.1361 - val_acc: 0.9778\n",
      "Epoch 189/200\n",
      "105/105 [==============================] - 0s 209us/step - loss: 0.1132 - acc: 0.9619 - val_loss: 0.1314 - val_acc: 0.9778\n",
      "Epoch 190/200\n",
      "105/105 [==============================] - 0s 237us/step - loss: 0.1119 - acc: 0.9619 - val_loss: 0.1228 - val_acc: 0.9778\n",
      "Epoch 191/200\n",
      "105/105 [==============================] - 0s 333us/step - loss: 0.1114 - acc: 0.9714 - val_loss: 0.1281 - val_acc: 0.9778\n",
      "Epoch 192/200\n",
      "105/105 [==============================] - 0s 285us/step - loss: 0.1131 - acc: 0.9619 - val_loss: 0.1296 - val_acc: 0.9778\n",
      "Epoch 193/200\n",
      "105/105 [==============================] - 0s 294us/step - loss: 0.1102 - acc: 0.9619 - val_loss: 0.1260 - val_acc: 0.9778\n",
      "Epoch 194/200\n",
      "105/105 [==============================] - 0s 275us/step - loss: 0.1097 - acc: 0.9619 - val_loss: 0.1297 - val_acc: 0.9778\n",
      "Epoch 195/200\n",
      "105/105 [==============================] - 0s 294us/step - loss: 0.1107 - acc: 0.9619 - val_loss: 0.1322 - val_acc: 0.9778\n",
      "Epoch 196/200\n",
      "105/105 [==============================] - 0s 295us/step - loss: 0.1092 - acc: 0.9619 - val_loss: 0.1246 - val_acc: 0.9778\n",
      "Epoch 197/200\n",
      "105/105 [==============================] - 0s 255us/step - loss: 0.1082 - acc: 0.9619 - val_loss: 0.1276 - val_acc: 0.9778\n",
      "Epoch 198/200\n",
      "105/105 [==============================] - 0s 206us/step - loss: 0.1077 - acc: 0.9619 - val_loss: 0.1209 - val_acc: 0.9778\n",
      "Epoch 199/200\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.1081 - acc: 0.9714 - val_loss: 0.1344 - val_acc: 0.9778\n",
      "Epoch 200/200\n",
      "105/105 [==============================] - 0s 277us/step - loss: 0.1093 - acc: 0.9619 - val_loss: 0.1271 - val_acc: 0.9778\n"
     ]
    }
   ],
   "source": [
    " history = model.fit(x_train , y_train, validation_data = (x_test ,y_test),epochs=200 , batch_size=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the training and validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 136us/step\n",
      "test_acc: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
